{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import logging\n",
    "from src.dataset_utils import load_data\n",
    "from src.models import create_model\n",
    "from src.defense import *\n",
    "from src.attack import *\n",
    "from src.helper import get_log_name\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Robust RAG')\n",
    "\n",
    "    # LLM settings\n",
    "    parser.add_argument('--model_name', type=str, default='mistral7b',choices=['llama3b','mistral7b','llama7b','gpt3.5'],help='model name')\n",
    "    parser.add_argument('--dataset_name', type=str, default='realtimeqa',choices=['realtimeqa-mc','realtimeqa','open_nq','biogen'],help='dataset name')\n",
    "    parser.add_argument('--top_k', type=int, default=10,help='top k retrieval')\n",
    "\n",
    "    # attack\n",
    "    parser.add_argument('--attack_method', type=str, default='none',choices=['none','Poison','PIA'], help='The attack method to use (Poison or Prompt Injection)')\n",
    "\n",
    "    # defense\n",
    "    parser.add_argument('--defense_method', type=str, default='keyword',choices=['none','voting','keyword','decoding'],help='The defense method to use')\n",
    "    parser.add_argument('--alpha', type=float, default=0.3, help='keyword filtering threshold alpha')\n",
    "    parser.add_argument('--beta', type=float, default=3.0, help='keyword filtering threshold beta')\n",
    "    parser.add_argument('--eta', type=float, default=0.0, help='decoding confidence threshold eta')\n",
    "\n",
    "    # certifcation\n",
    "    parser.add_argument('--corruption_size', type=int, default=1, help='The corruption size when considering certification/attack')\n",
    "    parser.add_argument('--subsample_iter', type=int, default=1, help='number of subsampled responses for decoding certifictaion')\n",
    "    # long gen certifcation # not really used in the paper\n",
    "    parser.add_argument('--temperature', type=float, default=1.0, help='The temperature for softmax')\n",
    "\n",
    "    # other\n",
    "    parser.add_argument('--debug', action = 'store_true', help='output debugging logging information')\n",
    "    parser.add_argument('--save_response', action = 'store_true', help='save the results for later analysis')\n",
    "    parser.add_argument('--use_cache', action = 'store_true', help='save/use cache responses from LLM')\n",
    "    parser.add_argument('--no_vanilla', action = 'store_true', help='do not run vanilla RAG')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    # LLM settings\n",
    "    model_name: str = 'llama3b'\n",
    "    dataset_name: str = 'open_nq'\n",
    "    top_k: int = 10\n",
    "\n",
    "    # attack\n",
    "    attack_method: str = 'Poison'\n",
    "\n",
    "    # defense\n",
    "    defense_method: str = 'keyword'\n",
    "    alpha: float = 0.3\n",
    "    beta: float = 3.0\n",
    "    eta: float = 0.0\n",
    "\n",
    "    # certification\n",
    "    corruption_size: int = 1\n",
    "    subsample_iter: int = 1\n",
    "    temperature: float = 1.0\n",
    "\n",
    "    # other\n",
    "    debug: bool = False\n",
    "    save_response: bool = False\n",
    "    use_cache: bool = False\n",
    "    no_vanilla: bool = False\n",
    "\n",
    "def main():\n",
    "    args = Args()\n",
    "    LOG_NAME = get_log_name(args)\n",
    "    logging_level = logging.DEBUG if args.debug else logging.INFO\n",
    "    \n",
    "    # create folder \n",
    "    os.makedirs(f'log',exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(#level=logging_level,\n",
    "        format=':::::::::::::: %(message)s'\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger('RRAG-main')\n",
    "    logger.setLevel(level=logging_level)\n",
    "    logger.addHandler(logging.FileHandler(f\"log/{LOG_NAME}.log\"))\n",
    "\n",
    "    logger.info(args)\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    # load data\n",
    "    data_tool = load_data(args.dataset_name,args.top_k)\n",
    "\n",
    "    if args.use_cache: # use/save cached responses from LLM\n",
    "        os.makedirs(f'cache/',exist_ok=True)\n",
    "        cache_path = f'cache/{args.model_name}-{args.dataset_name}-{args.top_k}.z'\n",
    "    else:\n",
    "        cache_path = None\n",
    "\n",
    "    # create LLM \n",
    "    if args.dataset_name == 'biogen':\n",
    "        llm = create_model(args.model_name,max_output_tokens=500)\n",
    "        # path for saving certification data\n",
    "        os.makedirs(f'result_certify',exist_ok=True)\n",
    "        certify_save_path = f'result_certify/{LOG_NAME}.json'\n",
    "        longgen = True\n",
    "    else:\n",
    "        llm = create_model(args.model_name,cache_path=cache_path)\n",
    "        certify_save_path = ''\n",
    "        longgen = False\n",
    "    no_defense = args.defense_method == 'none' or args.top_k<=0 # do not run defense\n",
    "\n",
    "    # wrap LLM with the defense class\n",
    "    if args.defense_method == 'voting': # majority voting\n",
    "        assert 'mc' in args.dataset_name\n",
    "        model = MajorityVoting(llm)\n",
    "    elif args.defense_method == 'keyword': # keyword aggregation\n",
    "        model = KeywordAgg(llm,relative_threshold=args.alpha,absolute_threshold=args.beta,longgen=longgen,certify_save_path=certify_save_path) \n",
    "    elif args.defense_method == 'decoding':\n",
    "        if args.eta>0 and not longgen:\n",
    "            logger.warning(f\"using non-zero eta {args.eta} for QA\")\n",
    "        eval_certify = len(certify_save_path)==0\n",
    "        model = DecodingAgg(llm,args,eval_certify=eval_certify,certify_save_path=certify_save_path)\n",
    "    else:\n",
    "        model = RRAG(llm) # base class\n",
    "\n",
    "    # init attack class\n",
    "    no_attack = args.attack_method == 'none' or args.top_k<=0 # do not run attack\n",
    "\n",
    "    if no_attack:\n",
    "        pass\n",
    "    elif args.attack_method == 'PIA':\n",
    "        if args.dataset_name == 'biogen':\n",
    "            attacker = PIALONG(top_k = args.top_k, poison_num=args.corruption_size, repeat=3, poison_order= \"backward\")\n",
    "        else:\n",
    "            attacker = PIA(top_k = args.top_k, poison_num=args.corruption_size, repeat=10, poison_order= \"backward\")\n",
    "    elif args.attack_method == 'Poison':\n",
    "        if args.dataset_name == 'biogen':\n",
    "            attacker = PoisonLONG(top_k = args.top_k, poison_num=args.corruption_size, repeat=3, poison_order= \"backward\")\n",
    "        else:\n",
    "            attacker = Poison(top_k = args.top_k, poison_num=args.corruption_size, repeat=10, poison_order= \"backward\")\n",
    "    else:\n",
    "        NotImplementedError\n",
    "\n",
    "    if not no_attack:\n",
    "        args.corruption_size = 0 # no certification for attack    # ad-hoc implementation -- tofix\n",
    "\n",
    "    defended_corr_cnt = 0\n",
    "    undefended_corr_cnt = 0\n",
    "    certify_cnt = 0\n",
    "    undefended_asr_cnt = 0\n",
    "    defended_asr_cnt = 0\n",
    "    corr_list = []\n",
    "    response_list = []\n",
    "    for data_item in tqdm(data_tool.data[:5]):\n",
    "       \n",
    "        # clean data_item\n",
    "        data_item = data_tool.process_data_item(data_item)\n",
    "        # attack\n",
    "        if not no_attack:\n",
    "            data_item = attacker.attack(data_item)\n",
    "        \n",
    "        # undefended\n",
    "        if not args.no_vanilla:\n",
    "            response_undefended = model.query_undefended(data_item)\n",
    "            undefended_corr = data_tool.eval_response(response_undefended,data_item)\n",
    "            undefended_corr_cnt += undefended_corr\n",
    "        else:\n",
    "            response_undefended = ''\n",
    "            undefended_corr = False\n",
    "\n",
    "        # undefended with asr\n",
    "        if not no_attack:\n",
    "            undefended_asr = data_tool.eval_response_asr(response_undefended,data_item)\n",
    "            undefended_asr_cnt += undefended_asr\n",
    "        # print(data_item)\n",
    "        response_list.append({\"query\":data_item[\"question\"],\"undefended\":response_undefended})\n",
    "        \n",
    "        # defended\n",
    "        if not no_defense: \n",
    "            print(f\"defense method: {args.defense_method}\")\n",
    "            response_defended,certificate = model.query(data_item,corruption_size=args.corruption_size)\n",
    "            print(\"done query\")\n",
    "            defended_corr = data_tool.eval_response(response_defended,data_item)\n",
    "            defended_corr_cnt += defended_corr\n",
    "            certify_cnt += (defended_corr and certificate)\n",
    "            if not no_attack:\n",
    "                defended_asr = data_tool.eval_response_asr(response_defended,data_item)\n",
    "                defended_asr_cnt += defended_asr\n",
    "            response_list.append({\"query\":data_item[\"question\"],\"defended\":response_defended})\n",
    "            corr_list.append(defended_corr and certificate)\n",
    "\n",
    "    logger.info(f'undefended_corr_cnt: {undefended_corr_cnt}')\n",
    "    logger.info(f'defended_corr_cnt: {defended_corr_cnt}')\n",
    "    logger.info(f'certify_cnt: {certify_cnt}')\n",
    "\n",
    "\n",
    "    if not no_attack:\n",
    "        logger.info(f'######################## ASR ########################')\n",
    "        logger.info(f'undefended_asr_cnt: {undefended_asr_cnt}')\n",
    "        logger.info(f'defended_asr_cnt: {defended_asr_cnt}')\n",
    "\n",
    "\n",
    "    # save for later analysis, currently used for biogen dataset \n",
    "    if args.save_response:\n",
    "        os.makedirs(f'result/{args.dataset_name}',exist_ok=True)\n",
    "        if args.defense_method == 'keyword':\n",
    "            with open(f'result/{LOG_NAME}.json','w') as f:\n",
    "                json.dump(response_list,f,indent=4)\n",
    "        else:\n",
    "            with open(f'result/{LOG_NAME}.json','w') as f:\n",
    "                json.dump(response_list,f,indent=4)\n",
    "\n",
    "\n",
    "    if args.use_cache:\n",
    "        llm.dump_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
