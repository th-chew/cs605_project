# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to models
model2path:
  llama: "meta-llama/Llama-3.2-3B-Instruct"
  qwen: "Qwen/Qwen2.5-3B-Instruct"
  rbft_llama: "loganchew/rbft_llama3"
  rbft_llama_v2: "loganchew/rbft_llama3_v2"
  rbft_qwen: "rbft/model/rbft_qwen"

save_dir: "log/"


# -------------------------------------------------Retrieval Settings------------------------------------------------#
# If set the name, the model path will be find in global paths
# retrieval_method: "e5"  # name or path of the retrieval model. 
# index_path: ~ # set automatically if not provided. 
# faiss_gpu: False # whether use gpu to hold index
# corpus_path: ~  # path to corpus in '.jsonl' format that store the documents

use_reranker: True # whether to use reranker
rerank_model_name: ~ # same as retrieval_method
rerank_model_path: ~ # path to reranker model, path will be automatically find in `retriever_model2path`
rerank_pooling_method: ~
rerank_topk: 5  # number of remain documents after reranking
rerank_max_length: 512
rerank_batch_size: 256 # batch size for reranker
rerank_use_fp16: True

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "llama" # name or path of the generator model
generator_max_input_len: 4096  # max length of the input
generator_batch_size: 2 # batch size for generation, invalid for vllm
gpu_memory_utilization: 0.5
generation_params:
  do_sample: False
  max_tokens: 32
use_fid: False # whether to use FID, only valid in encoder-decoder model

metrics: ['em','f1','acc','precision','recall']
metric_setting:
  retrieval_recall_topk: 5



