{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8276de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flashrag-dev --pre --upgrade\n",
    "!pip install -U datasets\n",
    "!pip install sentence-transformers==3.4.1\n",
    "!pip install vllm==0.7.3\n",
    "!pip install numpy==1.26.4\n",
    "!pip install flashinfer-python==0.2.5\n",
    "!pip install litellm==1.68.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m nltk.downloader stopwords\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1308f9a",
   "metadata": {},
   "source": [
    "### You will need to create a Hugging Face account to download the Llama 3.2 models. Ater creating an account, generate a token to be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token hf_4ZkXJYbqjWQdVZcXzYwzYwzYwzYwzYwzYwz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a236c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download meta-llama/Llama-3.2-3B-Instruct --local-dir meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173de8d",
   "metadata": {},
   "source": [
    "### Restart session after installing the packages because of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f66e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, argparse, datasets\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from eval.prompt import NaivePromptTemplate\n",
    "from eval.metrics import metric_dict\n",
    "from flashrag.config import Config\n",
    "from flashrag.utils import get_generator\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "def load_data(data_path, args):   \n",
    "    def load_psgs(item):\n",
    "        item['psgs'] = positive_psgs[item['qid']][:args.topk]\n",
    "        \n",
    "        # Attack\n",
    "        if args.attack_position == 'random':\n",
    "            sample_value = np.random.rand(len(item['psgs']))\n",
    "            for id, v in enumerate(sample_value):\n",
    "                if v <= args.tau: # random mode \n",
    "                    cur_attack = np.random.choice(['neg', 'nsy', 'cf']) \\\n",
    "                        if args.passage_attack == 'mix' \\\n",
    "                            else args.passage_attack\n",
    "                    item['psgs'][id] = psgs_dict[cur_attack][item['qid']][id]\n",
    "        \n",
    "        elif args.attack_position == 'top':\n",
    "            attack_topk = round(args.tau * len(item['psgs']))\n",
    "            for id in range(len(item['psgs'])):\n",
    "                if id < attack_topk: # top mode, only attack top psgs\n",
    "                    cur_attack = np.random.choice(['neg', 'nsy', 'cf']) \\\n",
    "                        if args.passage_attack == 'mix' \\\n",
    "                            else args.passage_attack\n",
    "                    item['psgs'][id] = psgs_dict[cur_attack][item['qid']][id]\n",
    "                else: # neglect bottom psgs\n",
    "                    break\n",
    "        \n",
    "        elif args.attack_position == 'bottom':\n",
    "            keep_topk = round((1 - args.tau) * len(item['psgs']))\n",
    "            for id in range(len(item['psgs'])):\n",
    "                if id < keep_topk: # bottom mode, neglect top psgs\n",
    "                    continue\n",
    "                else: # only attack bottom psgs\n",
    "                    cur_attack = np.random.choice(['neg', 'nsy', 'cf']) \\\n",
    "                        if args.passage_attack == 'mix' \\\n",
    "                            else args.passage_attack\n",
    "                    item['psgs'][id] = psgs_dict[cur_attack][item['qid']][id]\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                    \n",
    "                    \n",
    "        return item\n",
    "        \n",
    "    dataset = datasets.load_dataset(\n",
    "        'json',\n",
    "        data_files = data_path + \"/sample.json\",\n",
    "    )['train']\n",
    "    \n",
    "    \n",
    "    with open(data_path + \"/posp.json\") as fr:\n",
    "        positive_psgs = {}\n",
    "        for line in fr:\n",
    "            line = json.loads(line)\n",
    "            positive_psgs[line['qid']] = line['pos_psgs']\n",
    "    \n",
    "    neg_psgs, nsy_psgs, cf_psgs = {}, {}, {}\n",
    "    if args.passage_attack == 'neg' or args.passage_attack == 'mix':\n",
    "        with open(data_path + \"/negp.json\") as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line)\n",
    "                neg_psgs[line['qid']] = line['neg_psgs']\n",
    "                \n",
    "    if args.passage_attack == 'nsy' or args.passage_attack == 'mix':\n",
    "        with open(data_path + \"/nsyp.json\") as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line)\n",
    "                nsy_psgs[line['qid']] = line['nsy_psgs']\n",
    "                \n",
    "    if args.passage_attack == 'cf' or args.passage_attack == 'mix':\n",
    "        with open(data_path + \"/cfp.json\") as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line)\n",
    "                cf_psgs[line['qid']] = line['cf_psgs']\n",
    "    \n",
    "    psgs_dict = {\n",
    "        \"neg\" : neg_psgs,\n",
    "        \"nsy\" : nsy_psgs,\n",
    "        \"cf\" : cf_psgs\n",
    "    }\n",
    "    \n",
    "    dataset = dataset.map(load_psgs)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate(generator, prompt_template, query_dataset, rerank=False, top_k_rerank=5):\n",
    "\n",
    "    if rerank:\n",
    "        ranked_psgs = []\n",
    "        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')   \n",
    "\n",
    "        for query, psgs in zip(query_dataset['query'], query_dataset['psgs']):\n",
    "            pairs = [[query, psg] for psg in psgs]\n",
    "            \n",
    "            # Use sentence-transformers CrossEncoder for reranking\n",
    "            scores = reranker.predict(pairs)\n",
    "            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)                     \n",
    "            psgs = [psgs[i] for i in top_indices[:top_k_rerank]]\n",
    "            ranked_psgs.append(psgs)\n",
    "\n",
    "        psgs = ranked_psgs\n",
    "\n",
    "    else:\n",
    "        psgs = query_dataset['psgs']\n",
    "\n",
    "    input_prompts = [\n",
    "        prompt_template.get_string(\n",
    "            question=query,\n",
    "            retrieval_result=psgs\n",
    "        ) for query, psgs in zip(query_dataset['query'], psgs)\n",
    "    ] \n",
    "    \n",
    "    preds = generator.generate(input_prompts)\n",
    "    \n",
    "    return preds\n",
    "    \n",
    "\n",
    "def eval(args):\n",
    "    config_dict = {\"save_note\": \"eval\",\n",
    "                   \"gpu_id\": args.gpu_id, \n",
    "                }\n",
    "    config = Config(args.config_file, config_dict)\n",
    "    np.random.seed(config['seed'])\n",
    "    dataset = load_data(args.data_path, args)\n",
    "    scorers = [metric_dict[metric](config) for metric in config['metrics']]\n",
    "\n",
    "    use_robustrag = args.use_robustrag\n",
    "    rerank = args.rerank\n",
    "    topk_rerank = args.topk_rerank\n",
    "    \n",
    "    if use_robustrag:   \n",
    "        from src.models import create_model\n",
    "        from src.defense import KeywordAgg\n",
    "\n",
    "        preds = []     \n",
    "        print(\"Using RobustRAG\")        \n",
    "        llm = create_model(args.model_name,cache_path=None)\n",
    "        model = KeywordAgg(llm, relative_threshold=args.alpha, absolute_threshold=args.beta, longgen=False, certify_save_path='')\n",
    "        \n",
    "        if rerank:\n",
    "            ranked_psgs = []\n",
    "            reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
    "            for query, psgs in zip(dataset['query'], dataset['psgs']):\n",
    "                pairs = [[query, psg] for psg in psgs]\n",
    "                scores = reranker.predict(pairs)\n",
    "                top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)                     \n",
    "                psgs = [psgs[i] for i in top_indices[:topk_rerank]]\n",
    "                ranked_psgs.append(psgs)\n",
    "            dataset_psgs = ranked_psgs\n",
    "        else:\n",
    "            dataset_psgs = dataset['psgs']\n",
    "\n",
    "        for query, psgs in zip(dataset['query'], dataset_psgs):\n",
    "\n",
    "            query_hints = model.generate_query_hints(query, psgs)\n",
    "            query_prompt = llm.wrap_prompt_flashrag(query, [], hints=query_hints)\n",
    "            response = llm.query(query_prompt)\n",
    "            preds.append(response)\n",
    "        \n",
    "    else:\n",
    "        generator = get_generator(config)\n",
    "        prompt_template = NaivePromptTemplate(config)\n",
    "\n",
    "        preds = generate(generator, prompt_template, dataset, rerank, topk_rerank)\n",
    "\n",
    "    eval_results = [scorer.calculate_metric(preds, dataset['answer'])[0] for scorer in scorers]\n",
    "    print(eval_results)\n",
    "    \n",
    "    with open(args.output_file, 'w') as fw:\n",
    "        fw.write(json.dumps({'result':eval_results})+\"\\n\")\n",
    "        for i, (q, a, p) in enumerate(zip(dataset['query'], dataset['answer'], preds)):\n",
    "            fw.write(json.dumps({i:{'query':q, 'answer':a, 'pred':p}})+'\\n')\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    data_path: str = \"data/e5/test_data\"\n",
    "    gpu_id: str = \"0\"\n",
    "    topk: int = 5\n",
    "    rerank: bool = False\n",
    "    topk_rerank: int = 5\n",
    "    tau: float = 0.0\n",
    "    config_file: str = \"configs/eval_config.yaml\"\n",
    "    output_file: str = \"output/output.txt\"\n",
    "    attack_position: str = \"random\"\n",
    "    passage_attack: str = None\n",
    "    prompt_template: str = None\n",
    "\n",
    "    # RobustRAG parameters\n",
    "    model_name: str = 'llama3b'\n",
    "    defense_method: str = 'keyword'\n",
    "    alpha: float = 0.3\n",
    "    beta: float = 3.0\n",
    "    eta: float = 0.0\n",
    "    corruption_size: int = 1\n",
    "\n",
    "    use_robustrag: bool = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = Args()\n",
    "    args.data_path = \"data/e5/test\"    \n",
    "    args.gpu_id = \"0\"\n",
    "    args.topk = 10    \n",
    "    args.topk_rerank = 5\n",
    "    args.tau = 0.0    \n",
    "    args.config_file = \"configs/eval.yaml\"\n",
    "    args.attack_position = \"random\"\n",
    "    args.passage_attack = None\n",
    "    args.prompt_template = None\n",
    "\n",
    "    # Uncomment the different scenarios to evaluate.\n",
    "\n",
    "    # Vanilla RAG (Scenario 0)\n",
    "    args.use_robustrag = False\n",
    "    args.rerank = False\n",
    "    args.output_file = \"output/vanilla_rag.json\"\n",
    "\n",
    "    # Use Vanilla RAG with CrossEncoder reranking (Scenario 1). \n",
    "    # args.use_robustrag = False\n",
    "    # args.rerank = True\n",
    "    # args.output_file = \"output/crossencoder_rag.json\"\n",
    "\n",
    "    # Use RobustRAG with CrossEncoder reranking (Scenario 2). \n",
    "    # args.use_robustrag = True\n",
    "    # args.rerank = False\n",
    "    # args.output_file = \"output/crossencoder_robustrag.json\"\n",
    "\n",
    "    eval(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
