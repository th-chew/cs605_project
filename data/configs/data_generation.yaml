# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to models
model2path:
  llama3.2-3B-instruct: "meta-llama/Llama-3.2-3B-Instruct"

save_dir: "log/"

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "llama3.2-3B-instruct" # name or path of the generator model
generator_max_input_len: 8192  # max length of the input
generator_batch_size: 8 # batch size for generation, invalid for vllm
gpu_memory_utilization: 0.65
generation_params:
  do_sample: False
  max_tokens: 1024
use_fid: False # whether to use FID, only valid in encoder-decoder model
